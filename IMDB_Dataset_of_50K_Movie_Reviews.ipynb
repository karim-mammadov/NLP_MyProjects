{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPQM6EC4kBI7KU9UZIU5q5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karim-mammadov/NLP_MyProjects/blob/main/IMDB_Dataset_of_50K_Movie_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByzmSZRa4FLF"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "UfmQ23Xnx-Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ],
      "metadata": {
        "id": "2oviDspEyBa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile"
      ],
      "metadata": {
        "id": "SqEVY4e2yIDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile('/content/imdb-dataset-of-50k-movie-reviews.zip','r') as zip_ref:\n",
        "  zip_ref.extractall('/content')"
      ],
      "metadata": {
        "id": "-mA4ooPZyJhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/IMDB Dataset.csv')"
      ],
      "metadata": {
        "id": "rQYPFncLyKwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "67puEt4ZyRcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # HTML tag-ları çıxar\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Punctuation və xüsusi simvollar (əlavə təmizləmə)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Çoxlu boşluqları 1 boşluğa çevir\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Aşağı-hərf et\n",
        "    text = text.lower().strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "df['clean_review'] = df['review'].apply(clean_text)"
      ],
      "metadata": {
        "id": "DEk0GWY2yXYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['sentiment'] = le.fit_transform(df['sentiment'])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "yk_KN5C3yhAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('review',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "_qh9k5wZypkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ub3Koiu_y5Rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "texts = [\n",
        "    \"I love NLP\",\n",
        "    \"Tokenization is important\",\n",
        "    \"This is a sample sentence\"\n",
        "]\n",
        "\n",
        "#tokenize yarat\n",
        "\n",
        "tokenizer = Tokenizer(num_words=50, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# tokenlere cevir\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "print(\"Token indexleri:\", sequences)\n",
        "\n",
        "# Pad sequences (hamisini eyni uzunluga getir)\n",
        "\n",
        "padded = pad_sequences(sequences, maxlen=6, padding='post')\n",
        "print(\"Pad edilmis sequences:\\n\", padded)"
      ],
      "metadata": {
        "id": "9TajG1pqy1da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "metadata": {
        "id": "ohWgxix30HPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer yarat\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"‹OOV›\")\n",
        "tokenizer.fit_on_texts(df['clean_review' ])\n",
        "\n",
        "# Token-lara cevir\n",
        "sequences = tokenizer.texts_to_sequences(df['clean_review'])\n",
        "\n",
        "# Pad sequences\n",
        "max_len = 100 # her cümle 100 token uzunlugunda olacaq\n",
        "padded = pad_sequences (sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Naticani yoxla\n",
        "print(\"avvalca bir cümla: \\n\", df['clean_review'].iloc[0])"
      ],
      "metadata": {
        "id": "NpO6vk5v1h78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Pad edilmis tokenler:\\n\", padded[0])"
      ],
      "metadata": {
        "id": "XF7oWAGm1784"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding"
      ],
      "metadata": {
        "id": "1Wss_Ifb27bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n"
      ],
      "metadata": {
        "id": "GcNR96Z22wVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5000  #tokenizer.num_words\n",
        "embedding_dim = 50\n",
        "max_len = 100"
      ],
      "metadata": {
        "id": "JwtUmXBK3HQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = Embedding(input_dim = vocab_size, output_dim=embedding_dim)\n"
      ],
      "metadata": {
        "id": "0gMfoqnh3vGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "ZUfA3PXP33v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_sequence = np.array([[3, 10, 7, 25, 0, 0]])\n",
        "embedded_output = embedding_layer(demo_sequence)\n",
        "\n",
        "print(\"Shape:\", embedded_output.shape)\n",
        "print(\"Embedding vector:\\n\", embedded_output.numpy())\n",
        "#3D tensor: batch_size x max_len x embedding_dim"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SCi4wbVE343y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 50   # her soz ucun 50 olculu dense vector\n",
        "\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "\n",
        "# butun sequences-i embedding-e cevirmek\n",
        "\n",
        "embedded_output = embedding_layer(padded)  #  shape: (5000, 100, 50)\n",
        "embedded_output"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wtia-7HV4Vbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1-ci cumlenin embedding matrix-i\n",
        "\n",
        "print(\"1-ci cumle token sequence:\\n\", padded[0])\n",
        "print(\"Embedding output shape:\",  embedded_output[0].shape)\n",
        "print(\"Embedding vector ( 5 ilk olcu):\\n\", embedded_output[0, :5, :5].numpy()) # yalniz ilk 5 token x 5 olcu"
      ],
      "metadata": {
        "id": "vLMi4sfo5rbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "HnEOJBR96hQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([Embedding (input_dim=vocab_size, output_dim=embedding_dim),\n",
        "                      # embedding layer\n",
        "                      GlobalAveragePooling1D(),\n",
        "                      # average pooling\n",
        "                      Dense(24, activation='relu'),\n",
        "                      # hidden dense layer\n",
        "                      Dense (1, activation='sigmoid')\n",
        "                      # output: 0/1\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "49wPr_vn7ENf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(padded, df['sentiment'], epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "id": "t13YIZpI7ZAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "x = padded\n",
        "# token sequences\n",
        "y = df[ 'sentiment']. values\n",
        "# 1-ci cümlenin token sequence-i\n",
        "demo_sequence = x[0]\n",
        "# padded token sequence\n",
        "embedded_output = model.layers[0](demo_sequence.reshape(1, -1))\n",
        "# embedding output\n",
        "# Convert to numpy\n",
        "embedded_matrix = embedded_output.numpy()[0] # shape: (max_len, embedding_dim)\n",
        "\n",
        "# Heatmap (yalni] ilk 20 token va 10 ölçü vizual üçün)\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(embedded_matrix[:20, :10], annot=True, cmap='viridis', cbar=True)\n",
        "plt.title(\"1-ci cümlenin embedding matrix heatmap\")\n",
        "plt.xlabel(\"Embedding dimensions (first 10)\")\n",
        "plt.ylabel(\"Tokens (first 20)\")\n",
        "plt. show()"
      ],
      "metadata": {
        "id": "ttIbqaWn7lvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lS_IStbv7_BC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}